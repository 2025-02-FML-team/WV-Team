{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bba8009",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/2025-02-FML-team/WV-Team/blob/main/notebooks/05_class_balance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0f7cf03-adc0-4f43-af1e-cd17922b27be",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0f7cf03-adc0-4f43-af1e-cd17922b27be",
    "outputId": "f40eaa54-2436-49fe-eb5a-8d115a146fb0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/workspace/WV-Team/data')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DATA_DIR = Path('/content/unpacked/')\n",
    "    PACK_DIR = Path('/content/drive/My Drive/colab_drive/prepacked.zip')\n",
    "    shutil.copy(PACK_DIR, '/content/')\n",
    "    !unzip -o -q /content/prepacked.zip -d {DATA_DIR}\n",
    "else:\n",
    "    DATA_DIR= Path(os.path.join(os.getcwd(), \"../data/\")).resolve()\n",
    "DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7puDIfStRQGs",
   "metadata": {
    "id": "7puDIfStRQGs"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 13:58:41.503028: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm.notebook import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# CSV 로드 및 정리, 본인 경로에 맞게 변환\n",
    "CSV_PATH = DATA_DIR / 'whiskies_relabel.csv'\n",
    "IMAGE_SIZE = (256, 256)\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "tf.random.set_seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9zlQt6BNRWMF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "153e3daeac704ae8bd3a845e761f5c69",
      "02ebafc09f1a4e7ba20e2a1cf9fccfe9",
      "5087e63df54a4a7aa2c8fdbee60dda43",
      "9b6363585a834d68b424bd09693be95e",
      "5a37bae3144f4a5ebc361db1d02ecc15",
      "81589a93cdd848358ce75973668acdbc",
      "837cd80e9aec4987a0bfe36254a53c4c",
      "2c08b7b452be4dff98eb95d8a2aaeb91",
      "cbcf43f91a824e80aa0745e3cc57e6b3",
      "0bf88e29d24f4c1c8da7ca5f681233dc",
      "93a9cd26f71f4ff8ac955fcfc8208bef"
     ]
    },
    "id": "9zlQt6BNRWMF",
    "outputId": "5b0b58d4-38bb-475b-c9d5-05ed6e88ce97"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5279b9b50c34a6cacf91c37da7c18bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Images:   0%|          | 0/3042 [00:00<?, ?img/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(CSV_PATH, dtype={\"id\": str})\n",
    "df[\"id\"] = df[\"id\"].astype(str).str.strip().str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "df[\"category\"] = df[\"category\"].astype(str).str.strip()\n",
    "paths = [DATA_DIR / p for p in df[\"local_full_path\"]]\n",
    "\n",
    "bar = tqdm(paths, desc=\"Processing Images\", unit=\"img\")\n",
    "\n",
    "# 이미지 로드\n",
    "X_list = []\n",
    "for p in bar:\n",
    "    with Image.open(p) as im:\n",
    "        im = im.convert(\"RGB\")\n",
    "        im = im.resize(IMAGE_SIZE)\n",
    "        arr = np.asarray(im, dtype=np.uint8)\n",
    "        X_list.append(arr)\n",
    "X = np.stack(X_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "LZsc1nB0RYSi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LZsc1nB0RYSi",
    "outputId": "93df3f32-6828-48ef-95d1-75aa7a36bfd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (2585, 256, 256, 3)\n",
      "X_test : (457, 256, 256, 3)\n",
      "y_train 분포: [352 177 104  88  98  93  86 158 174 544  91 103]\n",
      "y_test  분포: [ 78  39  23  20  21  21  19  35  38 120  20  23]\n",
      "class mapping: {'Blended': 0, 'Bourbon': 1, 'Brandy': 2, 'Gin': 3, 'Liqueur': 4, 'Rum': 5, 'Rye': 6, 'SM_40_43': 7, 'SM_43_46': 8, 'SM_G46': 9, 'Tequila': 10, 'Vodka': 11}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# 라벨 인코딩\n",
    "labels = df[\"category\"].values\n",
    "le = LabelEncoder()\n",
    "y_int = le.fit_transform(labels)\n",
    "\n",
    "# test 분리\n",
    "X_rest, X_test, y_rest, y_test = train_test_split(\n",
    "    X, y_int,\n",
    "    test_size=0.15,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y_int\n",
    ")\n",
    "\n",
    "print(\"X_train:\", X_rest.shape)\n",
    "print(\"X_test :\", X_test.shape)\n",
    "\n",
    "print(\"y_train 분포:\", np.bincount(y_train))\n",
    "print(\"y_test  분포:\", np.bincount(y_test))\n",
    "\n",
    "CLASS_NUM = len(le.classes_)\n",
    "print(\"class mapping:\", dict(zip(le.classes_, range(CLASS_NUM))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0t6FOrHAVq0u",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0t6FOrHAVq0u",
    "outputId": "d7385606-d271-45d1-a6e7-d2bfd88c9784"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.4895833333333333, 1: 0.9736346516007532, 2: 1.6570512820512822, 3: 1.9583333333333333, 4: 1.7585034013605443, 5: 1.853046594982079, 6: 2.003875968992248, 7: 1.090717299578059, 8: 0.9904214559386973, 9: 0.3167892156862745, 10: 1.8937728937728937, 11: 1.6731391585760518}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights_array = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.array(range(CLASS_NUM)),\n",
    "    y=y_train,\n",
    ")\n",
    "\n",
    "class_weight_dict = {}\n",
    "\n",
    "i = 0;\n",
    "for weight in class_weights_array:\n",
    "    class_weight_dict[i] = weight\n",
    "    i += 1\n",
    "\n",
    "print(class_weight_dict)\n",
    "# 예: {0: 0.8, 1: 1.2, 2: 3.4, ...}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0zwOvd67RZPj",
   "metadata": {
    "id": "0zwOvd67RZPj"
   },
   "source": [
    "# 이전의 교훈\n",
    "1. model의 dense layer activation으로 gelu 이용\n",
    "2. batch noramlization 적용\n",
    "\n",
    "# 바꿔야할 것\n",
    "1. 불균형 해소\n",
    "1) other class 분해\\\n",
    "결과 : 여러개의 작은 subclass가 생겨남 노이즈가 줄었을 것이라고 추측\n",
    "2) 부족했던 rye, tequila(라이, 테킬라) 클래스의 샘플을 각각 50개씩 추가(증강)\\\n",
    "결과 : 일단 소수 클래스는 균등해짐 130개 가량...\n",
    "3) class별 weight 부과\\\n",
    "결과 : metric 차이가 있는지는 잘 관찰이 안됨.\n",
    "\n",
    "2. layer 탐색\n",
    "- Dense Layer\\\n",
    "일단 f1 score의 경우 경향성에 있어 차이는 크게 없었지만, score 자체는 0.05+정도 올라온 느낌\n",
    "다른 accuracy나 precision등에 있어서는 경향성의 차이도 생겼는데, 데이터 품질이 개선되어서 그런 것인지는 정확히 모르겠음. 둘 다 균형있게 보는 f1 score를 중심으로 보아 여전히 hl300x2가 좋은 것으로 보임. 다만 같은 2층짜리 구조에서 실험하거나 더 낮은 1층 구조에서 실험하는 것도 가능성이 있어보임.\n",
    "\n",
    "- Conv layer\n",
    "- Input layer\n",
    "- 아마도 input의 경우는 세로가 긴게 연산수를 크게 늘리지 않고서도 좋은 방법이라 사료됨..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a-SqhAtxR-X7",
   "metadata": {
    "id": "a-SqhAtxR-X7"
   },
   "outputs": [],
   "source": [
    "#03 노트북 코드++\n",
    "from sklearn.metrics import f1_score\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "class ControllerCallback(Callback):\n",
    "    def __init__(self, X_val, y_val, start_from_epoch=12, patient=3, tqdm=None):\n",
    "        super().__init__()\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.f1_scores = [] #this is for cumilating f1 per epoch\n",
    "        self.start_from_epoch = start_from_epoch\n",
    "        self.patient = patient\n",
    "        self.out = 0\n",
    "        self.best_f1 = -1\n",
    "        self.epochs = 0\n",
    "        self.tqdm = tqdm\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.epochs += 1\n",
    "        y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "        if self.y_val.ndim == 2:\n",
    "            y_true = np.argmax(self.y_val, axis=1)\n",
    "        else:\n",
    "            y_true = self.y_val\n",
    "\n",
    "        f1 = f1_score(y_true, y_pred, average='macro')\n",
    "        self.f1_scores.append(f1)\n",
    "        logs['val_macro_f1'] = f1\n",
    "\n",
    "        if f1 > self.best_f1:\n",
    "            self.best_f1 = f1\n",
    "\n",
    "        if 1 < epoch and epoch > self.start_from_epoch and f1 < self.f1_scores[-2]:\n",
    "            if not tqdm:\n",
    "                print(f\"\\nNon Improvement detected at EP : {epoch}, f1 : {f1}\")\n",
    "            self.out += 1\n",
    "\n",
    "        if self.tqdm:\n",
    "            self.tqdm.set_postfix(epochs=self.epochs, curr_f1=f1, best_f1=self.best_f1, strikes=self.out)\n",
    "\n",
    "        if self.out >= self.patient:\n",
    "            if not tqdm:\n",
    "                print(f\"\\nStopping at EP : {epoch}, f1 : {f1}\")\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "tfkknavsSKBM",
   "metadata": {
    "id": "tfkknavsSKBM"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.activations import gelu\n",
    "\n",
    "#the name keyword is just there to use kwargs, it's not actually used.\n",
    "def build_model(\n",
    "    hidden=[300, 300],\n",
    "    conv=[32, 64, 128],\n",
    "    conv_double=False,\n",
    "    input_dim=IMAGE_SIZE,\n",
    "    name=\"\"\n",
    "):\n",
    "    inputs = keras.Input(shape=(input_dim[0], input_dim[1], 3)) #근데 이거 조절 할라면 위에서도 바꿔 줘야하지 않나...\n",
    "\n",
    "    x = inputs\n",
    "    for cl in conv:\n",
    "        x = layers.Conv2D(cl, (3,3), activation='relu', padding='same')(x)\n",
    "        if conv_double:\n",
    "            x = layers.Conv2D(cl, (3,3), activation='relu', padding='same')(x)\n",
    "        x = layers.MaxPooling2D((2,2))(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "\n",
    "    for hl in hidden:\n",
    "        x = layers.Dense(hl)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation('gelu')(x)\n",
    "\n",
    "    outputs = layers.Dense(CLASS_NUM, activation='softmax')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=3e-4),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ND_KzSUYTUDD",
   "metadata": {
    "id": "ND_KzSUYTUDD"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "configs = [\n",
    "    #hidden layer\n",
    "    {\"name\": \"hl300x2_100x2_50x2\", \"hidden\": [300, 300, 100, 100, 50, 50]},\n",
    "    {\"name\": \"hl100x4\", \"hidden\": [100, 100, 100, 100]},\n",
    "    {\"name\": \"hl300x3\", \"hidden\": [300, 300, 300]},\n",
    "    {\"name\": \"hl300x2\", \"hidden\": [300, 300]},\n",
    "    {\"name\": \"hl400x2\", \"hidden\": [400, 400]},\n",
    "    {\"name\": \"hl200x2\", \"hidden\": [200, 200]},\n",
    "    {\"name\": \"hl400\", \"hidden\": [400]},\n",
    "    {\"name\": \"hl300\", \"hidden\": [300]},\n",
    "    {\"name\": \"hl200\", \"hidden\": [200]},\n",
    "\n",
    "    #conv layer\n",
    "    {\"name\": \"cl16_32_64\", \"conv\": [16, 32, 64]},\n",
    "    {\"name\": \"cl48_96_192\", \"conv\": [48, 96, 192]},\n",
    "    {\"name\": \"cld16_32_64\", \"conv\": [16, 32, 64], \"conv_double\": True},\n",
    "    {\"name\": \"cld48_96_192\", \"conv\": [48, 96, 192], \"conv_double\": True},\n",
    "\n",
    "    #input layer\n",
    "    #{\"name\": \"id320x192\", \"input_dim\": (320, 192)},\n",
    "    #{\"name\": \"id288x216\", \"input_dim\": (288, 216)},\n",
    "]\n",
    "\n",
    "cv_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9185094-a841-401d-a446-e8a6ac9f0eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "uEESi5nKWhxG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uEESi5nKWhxG",
    "outputId": "217f2c50-8718-443b-d35f-5b9619fe9c0e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5806ee429dc46fd9ba7af914ffa2241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Model Configurations:   0%|          | 0/13 [00:00<?, ?config/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d17145a0e70744eea1354fac363cf509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "st K-fold:   0%|          | 0/5 [00:00<?, ?fold/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1763906559.736414 2035910 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46745 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:a6:00.0, compute capability: 8.6\n",
      "2025-11-23 14:02:45.580552: I external/local_xla/xla/service/service.cc:163] XLA service 0x79aca401d880 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-11-23 14:02:45.580577: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2025-11-23 14:02:45.659803: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-11-23 14:02:46.148557: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91002\n",
      "2025-11-23 14:02:46.265370: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-23 14:02:46.265401: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-23 14:02:46.265410: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-23 14:02:46.265489: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-23 14:02:46.265500: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-23 14:02:46.265514: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-23 14:02:46.265571: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-23 14:02:46.265581: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-23 14:02:46.265589: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-23 14:02:46.265604: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-23 14:02:48.916368: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2919', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-11-23 14:02:49.118357: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2751', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-11-23 14:02:50.717336: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2667', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-11-23 14:02:51.831748: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2924', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-11-23 14:02:51.947665: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1192', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-11-23 14:02:52.013696: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2927', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-11-23 14:02:52.020044: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2930', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-11-23 14:02:52.043982: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_913', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-11-23 14:02:52.076381: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2924', 44 bytes spill stores, 44 bytes spill loads\n",
      "\n",
      "2025-11-23 14:02:52.122690: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2930', 44 bytes spill stores, 44 bytes spill loads\n",
      "\n",
      "2025-11-23 14:02:52.145107: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2927', 44 bytes spill stores, 44 bytes spill loads\n",
      "\n",
      "2025-11-23 14:02:52.210261: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2933', 160 bytes spill stores, 164 bytes spill loads\n",
      "\n",
      "2025-11-23 14:02:52.284132: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2927', 368 bytes spill stores, 368 bytes spill loads\n",
      "\n",
      "2025-11-23 14:02:52.294304: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2930', 368 bytes spill stores, 368 bytes spill loads\n",
      "\n",
      "2025-11-23 14:02:52.299702: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2924', 368 bytes spill stores, 368 bytes spill loads\n",
      "\n",
      "2025-11-23 14:02:52.490286: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2933', 440 bytes spill stores, 440 bytes spill loads\n",
      "\n",
      "I0000 00:00:1763906578.992969 2036344 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2025-11-23 14:03:01.058330: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-23 14:03:01.058368: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-23 14:03:01.058378: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-23 14:03:01.058454: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-23 14:03:01.058464: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-23 14:03:01.058478: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-23 14:03:02.419568: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2919', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-11-23 14:03:02.715000: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1192', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-11-23 14:03:02.727848: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2919', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-11-23 14:03:03.116521: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_913', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-11-23 14:03:03.675458: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_754', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> [CV Summary] hl300x2_100x2_50x2: f1_macro=0.4942 ± 0.0185, last_f1_macro=0.4795 ± 0.0110, acc=0.5323 ± 0.0090, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "653a7ead6d8b47b4a337d38684fc0c31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "st K-fold:   0%|          | 0/5 [00:00<?, ?fold/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 14:09:39.661922: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-23 14:09:39.957316: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2113', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-11-23 14:09:40.303086: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2115', 68 bytes spill stores, 68 bytes spill loads\n",
      "\n",
      "2025-11-23 14:09:44.994435: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_2113', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-11-23 14:09:45.276693: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_602', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> [CV Summary] hl100x4: f1_macro=0.5043 ± 0.0114, last_f1_macro=0.4909 ± 0.0134, acc=0.5261 ± 0.0087, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bedd82cd1dd4cc0806fc6208ff13888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "st K-fold:   0%|          | 0/5 [00:00<?, ?fold/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> [CV Summary] hl300x3: f1_macro=0.5083 ± 0.0182, last_f1_macro=0.4969 ± 0.0178, acc=0.5327 ± 0.0155, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b476147bde044e80adeb995733baf372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "st K-fold:   0%|          | 0/5 [00:00<?, ?fold/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> [CV Summary] hl300x2: f1_macro=0.4993 ± 0.0091, last_f1_macro=0.4919 ± 0.0115, acc=0.5250 ± 0.0143, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abfd2bb7d4ab4a5a87dfa9d3e92ec0f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "st K-fold:   0%|          | 0/5 [00:00<?, ?fold/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 14:27:11.428550: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-23 14:27:12.033129: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1223', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-11-23 14:27:12.040604: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1307', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-11-23 14:27:12.386841: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1312', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-11-23 14:27:12.470512: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1312', 44 bytes spill stores, 44 bytes spill loads\n",
      "\n",
      "2025-11-23 14:27:12.642888: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1312', 368 bytes spill stores, 368 bytes spill loads\n",
      "\n",
      "2025-11-23 14:27:16.754921: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1307', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-11-23 14:27:16.834086: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1307', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-11-23 14:27:16.860005: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1307', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-11-23 14:27:17.176328: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_450', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> [CV Summary] hl400x2: f1_macro=0.5178 ± 0.0056, last_f1_macro=0.5107 ± 0.0117, acc=0.5408 ± 0.0154, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8194b3fe0ae44416a710737e65f71c28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "st K-fold:   0%|          | 0/5 [00:00<?, ?fold/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 14:33:06.694538: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-23 14:33:06.694596: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-23 14:33:06.694644: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-23 14:33:06.694655: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-23 14:33:08.019902: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1307', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-11-23 14:33:08.226767: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_669', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "2025-11-23 14:33:08.524874: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1312', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-11-23 14:33:08.576725: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1312', 44 bytes spill stores, 44 bytes spill loads\n",
      "\n",
      "2025-11-23 14:33:08.729030: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1309', 68 bytes spill stores, 68 bytes spill loads\n",
      "\n",
      "2025-11-23 14:33:08.764618: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1312', 368 bytes spill stores, 368 bytes spill loads\n",
      "\n",
      "2025-11-23 14:33:12.915469: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-23 14:33:12.915534: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-23 14:33:14.054085: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1307', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-11-23 14:33:14.124640: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1307', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "2025-11-23 14:33:14.416247: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_450', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> [CV Summary] hl200x2: f1_macro=0.5216 ± 0.0152, last_f1_macro=0.5116 ± 0.0128, acc=0.5431 ± 0.0072, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2de8d1c41ce452a82dd81f980a1ce15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "st K-fold:   0%|          | 0/5 [00:00<?, ?fold/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> [CV Summary] hl400: f1_macro=0.5018 ± 0.0113, last_f1_macro=0.4981 ± 0.0124, acc=0.5215 ± 0.0100, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be5d1f24593540818004e81196293e2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "st K-fold:   0%|          | 0/5 [00:00<?, ?fold/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> [CV Summary] hl300: f1_macro=0.4974 ± 0.0222, last_f1_macro=0.4888 ± 0.0204, acc=0.5230 ± 0.0126, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea27a0ff41114c799fe7e186339863c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "st K-fold:   0%|          | 0/5 [00:00<?, ?fold/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> [CV Summary] hl200: f1_macro=0.5057 ± 0.0068, last_f1_macro=0.4973 ± 0.0080, acc=0.5238 ± 0.0079, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cf76a2b9a8b4192bad915fe7ab38582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "st K-fold:   0%|          | 0/5 [00:00<?, ?fold/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 14:56:34.118501: I external/local_xla/xla/service/gpu/autotuning/dot_search_space.cc:208] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs? Working around this by using the full hints set instead.\n",
      "2025-11-23 14:56:34.471647: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1307', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-11-23 14:56:34.959758: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1309', 68 bytes spill stores, 68 bytes spill loads\n",
      "\n",
      "2025-11-23 14:56:39.589103: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1307', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-11-23 14:56:39.653589: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1307', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-11-23 14:56:39.923105: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_450', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> [CV Summary] cl16_32_64: f1_macro=0.5073 ± 0.0037, last_f1_macro=0.4993 ± 0.0053, acc=0.5346 ± 0.0140, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "910beddf8253447fb9f61d283e1e1b7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "st K-fold:   0%|          | 0/5 [00:00<?, ?fold/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 15:01:24.644019: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1307', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "2025-11-23 15:01:33.173222: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1307', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "2025-11-23 15:01:33.429665: I external/local_xla/xla/stream_executor/cuda/subprocess_compilation.cc:346] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_450', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> [CV Summary] cl48_96_192: f1_macro=0.5080 ± 0.0025, last_f1_macro=0.4978 ± 0.0084, acc=0.5327 ± 0.0146, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a9b218dd8034ca981d2ba88d4d773d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "st K-fold:   0%|          | 0/5 [00:00<?, ?fold/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> [CV Summary] cld16_32_64: f1_macro=0.5136 ± 0.0095, last_f1_macro=0.5065 ± 0.0126, acc=0.5373 ± 0.0127, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd8a954b97ac4df2b58ad81d1f3f40f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "st K-fold:   0%|          | 0/5 [00:00<?, ?fold/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-23 15:16:53.836706: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-11-23 15:16:53.982696: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-11-23 15:17:03.082648: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-11-23 15:17:03.237931: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "2025-11-23 15:17:03.380088: E external/local_xla/xla/stream_executor/cuda/cuda_timer.cc:86] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> [CV Summary] cld48_96_192: f1_macro=0.5029 ± 0.0133, last_f1_macro=0.4930 ± 0.0126, acc=0.5168 ± 0.0109, \n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, precision_score\n",
    "\n",
    "# ----- 설정 값들 -----\n",
    "N_SPLITS   = 5      # k-fold 개수\n",
    "EPOCHS     = 50     # 최대 epoch\n",
    "BATCH_SIZE = 32\n",
    "#CONFIG_INDEX = 0\n",
    "\n",
    "skf = StratifiedKFold(\n",
    "    n_splits=N_SPLITS,\n",
    "    shuffle=True,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "bar_cfg = tqdm(configs, desc=\"Model Configurations\", unit=\"config\")\n",
    "\n",
    "for cfg in bar_cfg:\n",
    "    name = cfg[\"name\"]\n",
    "    bar_cfg.set_postfix(name=name)\n",
    "\n",
    "    fold_accuracies = []\n",
    "    fold_precisions = []\n",
    "    fold_f1s        = []\n",
    "    fold_last_f1s   = []\n",
    "\n",
    "    # k-fold 루프\n",
    "    bar_fold = tqdm(enumerate(skf.split(X_rest, y_rest), start=1), desc=\"st K-fold\", unit=\"fold\", total=N_SPLITS)\n",
    "    for fold_idx, (train_idx, val_idx) in bar_fold:\n",
    "        bar_fold.desc = f'st K-fold, fold:{fold_idx}'\n",
    "\n",
    "        X_tr, X_val = X_rest[train_idx], X_rest[val_idx]\n",
    "        y_tr, y_val = y_rest[train_idx], y_rest[val_idx]\n",
    "\n",
    "        # 모델 생성(컴파일도 여기서 진행!)\n",
    "        model = build_model(**cfg)\n",
    "        #model.summary() #debug\n",
    "\n",
    "        # f1 + early stopping + progress\n",
    "        controller = ControllerCallback(\n",
    "            X_val, y_val,\n",
    "            start_from_epoch=15,\n",
    "            patient=5,\n",
    "            tqdm=bar_fold\n",
    "        )\n",
    "\n",
    "        history = model.fit(\n",
    "            X_tr, y_tr,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=EPOCHS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            callbacks=[controller],\n",
    "            class_weight=class_weight_dict, #이렇게 하면 class weight를 줄 수 있음. 근데 그냥 이렇게 하고 끝낼 예정...\n",
    "            verbose=0,\n",
    "        )\n",
    "\n",
    "        # ---- 이 fold에서 metrics 계산 ----\n",
    "        # 1) loss / accuracy (evaluate)\n",
    "        loss, acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "\n",
    "        # 2) 예측값 가져와서 precision / f1 (macro) 계산\n",
    "        y_prob = model.predict(X_val, verbose=0)\n",
    "        y_pred = np.argmax(y_prob, axis=1)\n",
    "\n",
    "        y_true = y_val\n",
    "\n",
    "        precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        f1        = controller.best_f1\n",
    "        last_f1   = controller.f1_scores[-1]\n",
    "\n",
    "        fold_accuracies.append(acc)\n",
    "        fold_precisions.append(precision)\n",
    "        fold_f1s.append(f1)\n",
    "        fold_last_f1s.append(last_f1)\n",
    "\n",
    "        #now this actually helps\n",
    "        del model\n",
    "        model = None\n",
    "        gc.collect()\n",
    "\n",
    "    # ----- config별 평균/표준편차 정리 -----\n",
    "    cfg_row = {\n",
    "        \"name\": name,\n",
    "        \"acc_mean\":  float(np.mean(fold_accuracies)),\n",
    "        \"acc_std\":   float(np.std(fold_accuracies)),\n",
    "        \"prec_macro_mean\": float(np.mean(fold_precisions)),\n",
    "        \"prec_macro_std\":  float(np.std(fold_precisions)),\n",
    "        \"best_f1_macro_mean\":   float(np.mean(fold_f1s)),\n",
    "        \"best_f1_macro_std\":    float(np.std(fold_f1s)),\n",
    "        \"last_f1_macro_mean\":   float(np.mean(fold_last_f1s)),\n",
    "        \"last_f1_macro_std\":    float(np.std(fold_last_f1s)),\n",
    "    }\n",
    "\n",
    "    print(f\"\\n>>> [CV Summary] {name}: \"\n",
    "          f\"f1_macro={cfg_row['best_f1_macro_mean']:.4f} ± {cfg_row['best_f1_macro_std']:.4f}, \"\n",
    "          f\"last_f1_macro={cfg_row['last_f1_macro_mean']:.4f} ± {cfg_row['last_f1_macro_std']:.4f}, \"\n",
    "          f\"acc={cfg_row['acc_mean']:.4f} ± {cfg_row['acc_std']:.4f}, \")\n",
    "\n",
    "    cv_results.append(cfg_row)\n",
    "    #print(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "XorvY4lmYji3",
   "metadata": {
    "id": "XorvY4lmYji3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "K-Fold 결과 best_f1_macro_mean DESC\n",
      "              name  acc_mean  acc_std  prec_macro_mean  prec_macro_std  best_f1_macro_mean  best_f1_macro_std  last_f1_macro_mean  last_f1_macro_std\n",
      "           hl200x2  0.543133 0.007175         0.535630        0.017408            0.521618           0.015233            0.511623           0.012777\n",
      "           hl400x2  0.540812 0.015357         0.530896        0.018572            0.517823           0.005596            0.510663           0.011749\n",
      "       cld16_32_64  0.537331 0.012737         0.542202        0.012502            0.513639           0.009467            0.506462           0.012569\n",
      "           hl300x3  0.532689 0.015503         0.538928        0.025084            0.508318           0.018231            0.496925           0.017756\n",
      "       cl48_96_192  0.532689 0.014557         0.522111        0.009873            0.507969           0.002480            0.497796           0.008356\n",
      "        cl16_32_64  0.534623 0.013980         0.526796        0.009055            0.507297           0.003715            0.499262           0.005302\n",
      "             hl200  0.523791 0.007871         0.506863        0.008927            0.505678           0.006828            0.497283           0.008025\n",
      "           hl100x4  0.526112 0.008736         0.563351        0.028253            0.504316           0.011431            0.490887           0.013414\n",
      "      cld48_96_192  0.516828 0.010901         0.521709        0.018320            0.502891           0.013329            0.492996           0.012635\n",
      "             hl400  0.521470 0.009983         0.513594        0.014741            0.501767           0.011271            0.498056           0.012418\n",
      "           hl300x2  0.524952 0.014350         0.515732        0.016805            0.499309           0.009051            0.491890           0.011523\n",
      "             hl300  0.523017 0.012630         0.500460        0.022488            0.497361           0.022239            0.488763           0.020409\n",
      "hl300x2_100x2_50x2  0.532302 0.009023         0.561444        0.011310            0.494194           0.018493            0.479541           0.011033\n"
     ]
    }
   ],
   "source": [
    "cv_df = pd.DataFrame(cv_results).sort_values(\"best_f1_macro_mean\", ascending=False)\n",
    "print(\"\\nK-Fold 결과 best_f1_macro_mean DESC\")\n",
    "print(cv_df.to_string(index=False))\n",
    "\n",
    "kfold_result_csv_path = DATA_DIR / \"kfold_result_class.csv\"\n",
    "\n",
    "cv_df.to_csv(kfold_result_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d21394-0eed-45d3-9a16-38d61a12d4ed",
   "metadata": {},
   "source": [
    "# Test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11bba261-22cd-449a-a097-77ff383d03d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 90ms/step - accuracy: 0.2665 - loss: 2.0524 - val_macro_f1: 0.0906\n",
      "Epoch 2/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.5095 - loss: 1.3479 - val_macro_f1: 0.2312\n",
      "Epoch 3/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.7377 - loss: 0.7889 - val_macro_f1: 0.1636\n",
      "Epoch 4/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - accuracy: 0.8526 - loss: 0.4692 - val_macro_f1: 0.1976\n",
      "Epoch 5/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.9327 - loss: 0.2517 - val_macro_f1: 0.2972\n",
      "Epoch 6/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.9718 - loss: 0.1511 - val_macro_f1: 0.1963\n",
      "Epoch 7/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.9741 - loss: 0.1282 - val_macro_f1: 0.2236\n",
      "Epoch 8/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.9752 - loss: 0.1045 - val_macro_f1: 0.2727\n",
      "Epoch 9/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.9857 - loss: 0.0785 - val_macro_f1: 0.4110\n",
      "Epoch 10/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.9934 - loss: 0.0500 - val_macro_f1: 0.4159\n",
      "Epoch 11/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.9988 - loss: 0.0253 - val_macro_f1: 0.4630\n",
      "Epoch 12/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.9961 - loss: 0.0229 - val_macro_f1: 0.4391\n",
      "Epoch 13/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.9965 - loss: 0.0257 - val_macro_f1: 0.4031\n",
      "Epoch 14/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.9973 - loss: 0.0216 - val_macro_f1: 0.4157\n",
      "Epoch 15/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.9988 - loss: 0.0101 - val_macro_f1: 0.5089\n",
      "Epoch 16/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0041 - val_macro_f1: 0.5183\n",
      "Epoch 17/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0030 - val_macro_f1: 0.5264\n",
      "Epoch 18/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0024 - val_macro_f1: 0.5171\n",
      "Epoch 19/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 1.0000 - loss: 0.0021 - val_macro_f1: 0.5145\n",
      "Epoch 20/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0018 - val_macro_f1: 0.5173\n",
      "Epoch 21/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0016 - val_macro_f1: 0.5190\n",
      "Epoch 22/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0015 - val_macro_f1: 0.5175\n",
      "Epoch 23/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 28ms/step - accuracy: 1.0000 - loss: 0.0013 - val_macro_f1: 0.5175\n",
      "Epoch 24/50\n",
      "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 1.0000 - loss: 0.0012 - val_macro_f1: 0.5167\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성\n",
    "model = build_model(**{\n",
    "    \"name\": \"model_v2\",\n",
    "    \"hidden\": [200, 200],\n",
    "    \"conv\": [16, 32, 64], \n",
    "    \"conv_double\": True\n",
    "})\n",
    "\n",
    "# f1 + early stopping + progress\n",
    "controller = ControllerCallback(\n",
    "    X_test, y_test,\n",
    "    start_from_epoch=15,\n",
    "    patient=5\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    X_rest, y_rest,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[controller],\n",
    "    class_weight=class_weight_dict,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "y_pred = np.argmax(model.predict(X_test, verbose=0), axis=1)\n",
    "\n",
    "precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "f1        = controller.best_f1\n",
    "last_f1   = controller.f1_scores[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1836dd0-6767-4fc1-9382-29bc00a19ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.5369886692428985, f1: 0.5264253002609424, last_f1:0.5167420002441089\n"
     ]
    }
   ],
   "source": [
    "print(f'precision: {precision}, f1: {f1}, last_f1:{last_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb2333f9-fdba-4bb6-bb9a-2b5d59a54abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(DATA_DIR / \"model_v2.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ea8269-b328-41dd-8806-faf1836b18d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (wv-team-venv)",
   "language": "python",
   "name": "wv-team-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "02ebafc09f1a4e7ba20e2a1cf9fccfe9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_81589a93cdd848358ce75973668acdbc",
      "placeholder": "​",
      "style": "IPY_MODEL_837cd80e9aec4987a0bfe36254a53c4c",
      "value": "Processing Images: 100%"
     }
    },
    "0bf88e29d24f4c1c8da7ca5f681233dc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "153e3daeac704ae8bd3a845e761f5c69": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_02ebafc09f1a4e7ba20e2a1cf9fccfe9",
       "IPY_MODEL_5087e63df54a4a7aa2c8fdbee60dda43",
       "IPY_MODEL_9b6363585a834d68b424bd09693be95e"
      ],
      "layout": "IPY_MODEL_5a37bae3144f4a5ebc361db1d02ecc15"
     }
    },
    "2c08b7b452be4dff98eb95d8a2aaeb91": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5087e63df54a4a7aa2c8fdbee60dda43": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2c08b7b452be4dff98eb95d8a2aaeb91",
      "max": 3042,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cbcf43f91a824e80aa0745e3cc57e6b3",
      "value": 3042
     }
    },
    "5a37bae3144f4a5ebc361db1d02ecc15": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "81589a93cdd848358ce75973668acdbc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "837cd80e9aec4987a0bfe36254a53c4c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "93a9cd26f71f4ff8ac955fcfc8208bef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9b6363585a834d68b424bd09693be95e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0bf88e29d24f4c1c8da7ca5f681233dc",
      "placeholder": "​",
      "style": "IPY_MODEL_93a9cd26f71f4ff8ac955fcfc8208bef",
      "value": " 3042/3042 [00:29&lt;00:00, 118.56img/s]"
     }
    },
    "cbcf43f91a824e80aa0745e3cc57e6b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
