{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2025-02-FML-team/WV-Team/blob/train-pipeline/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "k 브랜치용 학습 파이프라인 (PyTorch)\n",
        "- 목적: \"k\" 브랜치에 들어갈 재현 가능한 이미지 분류 학습 코드\n",
        "- 입력: uploads 폴더(또는 CSV에서 지정한 경로). 예: photos/..., whiskies_recategorized.csv\n",
        "- 출력: 체크포인트(.pt), 학습 로그, confusion matrix (numpy)\n",
        "\n",
        "사용법 예시:\n",
        "$ pip install -r requirements.txt\n",
        "$ python k_branch_training.py --csv /mnt/data/whiskies_recategorized.csv --img-root /mnt/data/photos --epochs 30\n",
        "\n",
        "작성자: (자동생성)\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "import torchvision.transforms as T\n",
        "import torchvision.models as models\n",
        "\n",
        "# -------------- Dataset --------------\n",
        "class WhiskyImageDataset(Dataset):\n",
        "    def __init__(self, df, img_root, transform=None, img_col='local_full_path', label_col='category'):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.img_root = img_root\n",
        "        self.transform = transform\n",
        "        self.img_col = img_col\n",
        "        self.label_col = label_col\n",
        "        self.classes = sorted(self.df[self.label_col].unique())\n",
        "        self.class_to_idx = {c:i for i,c in enumerate(self.classes)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.loc[idx]\n",
        "        rel_path = row[self.img_col]\n",
        "        img_path = os.path.join(self.img_root, rel_path) if not os.path.isabs(rel_path) else rel_path\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        label = self.class_to_idx[row[self.label_col]]\n",
        "        return img, label\n",
        "\n",
        "# -------------- Utilities --------------\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def stratified_split(df, label_col='category', train_frac=0.64, val_frac=0.16, test_frac=0.2, seed=42):\n",
        "    assert abs(train_frac + val_frac + test_frac - 1.0) < 1e-6\n",
        "    np.random.seed(seed)\n",
        "    train_idx, val_idx, test_idx = [], [], []\n",
        "    for _, grp in df.groupby(label_col):\n",
        "        n = len(grp)\n",
        "        idxs = grp.index.to_numpy()\n",
        "        np.random.shuffle(idxs)\n",
        "        n_train = int(np.floor(train_frac * n))\n",
        "        n_val = int(np.floor(val_frac * n))\n",
        "        train_idx.extend(idxs[:n_train].tolist())\n",
        "        val_idx.extend(idxs[n_train:n_train+n_val].tolist())\n",
        "        test_idx.extend(idxs[n_train+n_val:].tolist())\n",
        "    return df.loc[train_idx].reset_index(drop=True), df.loc[val_idx].reset_index(drop=True), df.loc[test_idx].reset_index(drop=True)\n",
        "\n",
        "\n",
        "# -------------- Model helpers --------------\n",
        "\n",
        "def get_model(num_classes, model_name='resnet50', pretrained=True):\n",
        "    if model_name == 'resnet50':\n",
        "        m = models.resnet50(pretrained=pretrained)\n",
        "        in_f = m.fc.in_features\n",
        "        m.fc = nn.Linear(in_f, num_classes)\n",
        "    elif model_name == 'resnet18':\n",
        "        m = models.resnet18(pretrained=pretrained)\n",
        "        in_f = m.fc.in_features\n",
        "        m.fc = nn.Linear(in_f, num_classes)\n",
        "    elif model_name == 'efficientnet_b0':\n",
        "        m = models.efficientnet_b0(pretrained=pretrained)\n",
        "        in_f = m.classifier[1].in_features\n",
        "        m.classifier[1] = nn.Linear(in_f, num_classes)\n",
        "    else:\n",
        "        raise ValueError('Unsupported model_name')\n",
        "    return m\n",
        "\n",
        "\n",
        "# -------------- Training loop --------------\n",
        "\n",
        "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for imgs, labels in dataloader:\n",
        "        imgs = imgs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * imgs.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += imgs.size(0)\n",
        "    return running_loss / total, correct / total\n",
        "\n",
        "\n",
        "def validate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in dataloader:\n",
        "            imgs = imgs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * imgs.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += imgs.size(0)\n",
        "            all_preds.extend(preds.cpu().numpy().tolist())\n",
        "            all_labels.extend(labels.cpu().numpy().tolist())\n",
        "    avg_loss = running_loss / total\n",
        "    acc = correct / total\n",
        "    return avg_loss, acc, np.array(all_preds), np.array(all_labels)\n",
        "\n",
        "\n",
        "# -------------- Main --------------\n",
        "\n",
        "def main(args):\n",
        "    set_seed(args.seed)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() and not args.no_cuda else 'cpu')\n",
        "\n",
        "    # Load CSV\n",
        "    df = pd.read_csv(args.csv)\n",
        "    # Ensure the expected columns exist\n",
        "    assert args.img_col in df.columns and args.label_col in df.columns, f\"CSV must contain columns: {args.img_col}, {args.label_col}\"\n",
        "\n",
        "    # Optional: filter/remove classes with too few samples (commented out)\n",
        "    if args.min_samples > 0:\n",
        "        counts = df[args.label_col].value_counts()\n",
        "        keep = counts[counts >= args.min_samples].index.tolist()\n",
        "        df = df[df[args.label_col].isin(keep)].reset_index(drop=True)\n",
        "\n",
        "    # Stratified split\n",
        "    train_df, val_df, test_df = stratified_split(df, label_col=args.label_col,\n",
        "                                                 train_frac=args.train_frac, val_frac=args.val_frac, test_frac=args.test_frac,\n",
        "                                                 seed=args.seed)\n",
        "    print(f\"Split sizes: train={len(train_df)}, val={len(val_df)}, test={len(test_df)}\")\n",
        "\n",
        "    # Transforms\n",
        "    train_transform = T.Compose([\n",
        "        T.Resize((args.img_size, args.img_size)),\n",
        "        T.RandomHorizontalFlip(),\n",
        "        T.RandomVerticalFlip(),\n",
        "        T.RandomRotation(20),\n",
        "        T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.02),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "    ])\n",
        "    val_transform = T.Compose([\n",
        "        T.Resize((args.img_size, args.img_size)),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "    ])\n",
        "\n",
        "    # Datasets\n",
        "    train_ds = WhiskyImageDataset(train_df, args.img_root, transform=train_transform,\n",
        "                                  img_col=args.img_col, label_col=args.label_col)\n",
        "    val_ds = WhiskyImageDataset(val_df, args.img_root, transform=val_transform,\n",
        "                                img_col=args.img_col, label_col=args.label_col)\n",
        "    test_ds = WhiskyImageDataset(test_df, args.img_root, transform=val_transform,\n",
        "                                 img_col=args.img_col, label_col=args.label_col)\n",
        "\n",
        "    num_classes = len(train_ds.classes)\n",
        "    print('Classes:', train_ds.classes)\n",
        "\n",
        "    # Weighted sampler to mitigate class imbalance\n",
        "    if args.use_weighted_sampler:\n",
        "        counts = train_df[args.label_col].value_counts().to_dict()\n",
        "        weights = [1.0 / counts[c] for c in train_df[args.label_col]]\n",
        "        sampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n",
        "        shuffle = False\n",
        "    else:\n",
        "        sampler = None\n",
        "        shuffle = True\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=args.batch_size, sampler=sampler, shuffle=shuffle, num_workers=4, pin_memory=True)\n",
        "    val_loader = DataLoader(val_ds, batch_size=args.batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "    test_loader = DataLoader(test_ds, batch_size=args.batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "    # Model\n",
        "    model = get_model(num_classes=num_classes, model_name=args.model, pretrained=not args.no_pretrained)\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Loss + optimizer\n",
        "    if args.class_weighting:\n",
        "        # compute class weights (inverse frequency)\n",
        "        class_counts = train_df[args.label_col].value_counts().reindex(train_ds.classes).fillna(0).values\n",
        "        class_weights = torch.tensor(1.0 / (class_counts + 1e-6), dtype=torch.float32)\n",
        "        class_weights = class_weights / class_weights.sum() * len(class_weights)\n",
        "        criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
        "    else:\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)\n",
        "\n",
        "    best_val_acc = 0.0\n",
        "    for epoch in range(1, args.epochs + 1):\n",
        "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        val_loss, val_acc, _, _ = validate(model, val_loader, criterion, device)\n",
        "        scheduler.step()\n",
        "\n",
        "        print(f\"Epoch {epoch:02d}/{args.epochs} | train_loss: {train_loss:.4f} acc: {train_acc:.4f} | val_loss: {val_loss:.4f} acc: {val_acc:.4f}\")\n",
        "\n",
        "        # save best\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            os.makedirs(args.save_dir, exist_ok=True)\n",
        "            save_path = os.path.join(args.save_dir, f'best_model_epoch{epoch}_acc{val_acc:.4f}.pt')\n",
        "            torch.save({'model_state_dict': model.state_dict(), 'epoch': epoch, 'val_acc': val_acc}, save_path)\n",
        "            print('Saved', save_path)\n",
        "\n",
        "    # Final evaluation on test set\n",
        "    test_loss, test_acc, preds, labels = validate(model, test_loader, criterion, device)\n",
        "    print(f\"Test: loss={test_loss:.4f} acc={test_acc:.4f}\")\n",
        "\n",
        "    # Save predictions\n",
        "    out_df = test_df.copy()\n",
        "    out_df['pred_idx'] = preds\n",
        "    out_df['label_idx'] = labels\n",
        "    out_df['pred'] = out_df['pred_idx'].map(lambda x: train_ds.classes[x])\n",
        "    out_df['label'] = out_df['label_idx'].map(lambda x: train_ds.classes[x])\n",
        "    out_csv = os.path.join(args.save_dir, 'test_predictions.csv')\n",
        "    out_df.to_csv(out_csv, index=False)\n",
        "    print('Saved predictions to', out_csv)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--csv', type=str, default='/mnt/data/whiskies_recategorized.csv', help='CSV file with local_full_path and category columns')\n",
        "    parser.add_argument('--img-root', type=str, default='/mnt/data/photos', help='Root folder that images are relative to')\n",
        "    parser.add_argument('--img-col', type=str, default='local_full_path')\n",
        "    parser.add_argument('--label-col', type=str, default='category')\n",
        "    parser.add_argument('--img-size', type=int, default=256)\n",
        "    parser.add_argument('--batch-size', type=int, default=32)\n",
        "    parser.add_argument('--epochs', type=int, default=20)\n",
        "    parser.add_argument('--lr', type=float, default=3e-4)\n",
        "    parser.add_argument('--weight-decay', type=float, default=1e-4)\n",
        "    parser.add_argument('--model', type=str, default='resnet50', choices=['resnet18','resnet50','efficientnet_b0'])\n",
        "    parser.add_argument('--no-pretrained', action='store_true')\n",
        "    parser.add_argument('--use-weighted-sampler', action='store_true')\n",
        "    parser.add_argument('--class-weighting', action='store_true')\n",
        "    parser.add_argument('--min-samples', type=int, default=0, help='Remove classes with fewer than this many samples')\n",
        "    parser.add_argument('--train-frac', type=float, default=0.64)\n",
        "    parser.add_argument('--val-frac', type=float, default=0.16)\n",
        "    parser.add_argument('--test-frac', type=float, default=0.20)\n",
        "    parser.add_argument('--save-dir', type=str, default='./k_branch_outputs')\n",
        "    parser.add_argument('--seed', type=int, default=42)\n",
        "    parser.add_argument('--no-cuda', action='store_true')\n",
        "    args = parser.parse_args()\n",
        "    main(args)\n"
      ],
      "metadata": {
        "id": "hCoLQqe33Irg",
        "outputId": "2d7ccfe5-2125-47f6-bf35-4753f2a40652",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] [--csv CSV] [--img-root IMG_ROOT]\n",
            "                                [--img-col IMG_COL] [--label-col LABEL_COL]\n",
            "                                [--img-size IMG_SIZE]\n",
            "                                [--batch-size BATCH_SIZE] [--epochs EPOCHS]\n",
            "                                [--lr LR] [--weight-decay WEIGHT_DECAY]\n",
            "                                [--model {resnet18,resnet50,efficientnet_b0}]\n",
            "                                [--no-pretrained] [--use-weighted-sampler]\n",
            "                                [--class-weighting]\n",
            "                                [--min-samples MIN_SAMPLES]\n",
            "                                [--train-frac TRAIN_FRAC]\n",
            "                                [--val-frac VAL_FRAC] [--test-frac TEST_FRAC]\n",
            "                                [--save-dir SAVE_DIR] [--seed SEED]\n",
            "                                [--no-cuda]\n",
            "colab_kernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-8c4b0f9a-9251-4edc-a63b-75773adc84e4.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "2",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}